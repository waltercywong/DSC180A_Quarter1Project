{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import gzip\n",
    "from scipy.sparse import coo_matrix\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from scipy.stats import binned_statistic_2d\n",
    "from collections import defaultdict\n",
    "\n",
    "from numpy.linalg import eig\n",
    "from torch_geometric.utils import (get_laplacian, to_scipy_sparse_matrix, to_undirected, to_dense_adj)\n",
    "from scipy.sparse.linalg import eigsh\n",
    "\n",
    "import pymetis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Raw Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_dir = '../data/DigIC_dataset/'\n",
    "clean_data_dir = '../data/clean_data/'\n",
    "design = 'xbar'\n",
    "n_variants = 13\n",
    "\n",
    "sample_names = []\n",
    "corresponding_design = []\n",
    "corresponding_variant = []\n",
    "for idx in range(n_variants):\n",
    "    sample_name = raw_data_dir + design + '/' + str(idx + 1) + '/'\n",
    "    sample_names.append(sample_name)\n",
    "    corresponding_design.append(design)\n",
    "    corresponding_variant.append(idx + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_fn = raw_data_dir + 'cells.json.gz'\n",
    "with gzip.open(cells_fn, 'r') as fin:\n",
    "    cell_data = json.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widths = []\n",
    "heights = []\n",
    "for idx in range(len(cell_data)):\n",
    "    width = cell_data[idx]['width']\n",
    "    height = cell_data[idx]['height']\n",
    "    widths.append(width)\n",
    "    heights.append(height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_fn = raw_data_dir + 'cells.json.gz'\n",
    "with gzip.open(cells_fn, 'r') as fin:\n",
    "    cell_data = json.load(fin)\n",
    "\n",
    "widths = []\n",
    "heights = []\n",
    "for idx in range(len(cell_data)):\n",
    "    width = cell_data[idx]['width']\n",
    "    height = cell_data[idx]['height']\n",
    "    widths.append(width)\n",
    "    heights.append(height)\n",
    "\n",
    "widths = np.array(widths)\n",
    "heights = np.array(heights)\n",
    "\n",
    "min_cell_width = np.min(widths)\n",
    "max_cell_width = np.max(widths)\n",
    "min_cell_height = np.min(heights)\n",
    "max_cell_height = np.max(heights)\n",
    "\n",
    "# Scale all widths and heights of each cell type from 0 to 1\n",
    "widths = (widths - min_cell_width) / (max_cell_width - min_cell_width)\n",
    "heights = (heights - min_cell_height) / (max_cell_height - min_cell_height)\n",
    "\n",
    "# For each cell map the input and output pins\n",
    "cell_to_edge_dict = {item['id']:{inner_item['id']: inner_item['dir'] for inner_item in item['terms']} for item in cell_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in range(n_variants):\n",
    "    folder = sample_names[sample]\n",
    "    design = corresponding_design[sample]\n",
    "    instances_nets_fn = folder + design + '.json.gz'\n",
    "\n",
    "    print('--------------------------------------------------')\n",
    "    print('Folder:', folder)\n",
    "    print('Design:', design)\n",
    "    print('Instances & nets info:', instances_nets_fn)\n",
    "\n",
    "    with gzip.open(instances_nets_fn, 'r') as fin:\n",
    "        instances_nets_data = json.load(fin)\n",
    "\n",
    "    instances = instances_nets_data['instances']\n",
    "    nets = instances_nets_data['nets']\n",
    "\n",
    "    inst_to_cell = {item['id']:item['cell'] for item in instances}\n",
    "\n",
    "    num_instances = len(instances)\n",
    "    num_nets = len(nets)\n",
    "\n",
    "    print('Number of instances:', num_instances)\n",
    "    print('Number of nets:', num_nets)\n",
    "\n",
    "    xloc_list = [instances[idx]['xloc'] for idx in range(num_instances)]\n",
    "    yloc_list = [instances[idx]['yloc'] for idx in range(num_instances)]\n",
    "    cell = [instances[idx]['cell'] for idx in range(num_instances)]\n",
    "    cell_width = [widths[cell[idx]] for idx in range(num_instances)]\n",
    "    cell_height = [heights[cell[idx]] for idx in range(num_instances)]\n",
    "    orient = [instances[idx]['orient'] for idx in range(num_instances)]\n",
    "    \n",
    "    x_min = min(xloc_list)\n",
    "    x_max = max(xloc_list)\n",
    "    y_min = min(yloc_list)\n",
    "    y_max = max(yloc_list)\n",
    "\n",
    "    X = np.expand_dims(np.array(xloc_list), axis = 1)\n",
    "    Y = np.expand_dims(np.array(yloc_list), axis = 1)\n",
    "    X = (X - x_min) / (x_max - x_min)\n",
    "    Y = (Y - y_min) / (y_max - y_min)\n",
    "\n",
    "    cell = np.expand_dims(np.array(cell), axis = 1)\n",
    "    cell_width = np.expand_dims(np.array(cell_width), axis = 1)\n",
    "    cell_height = np.expand_dims(np.array(cell_height), axis = 1)\n",
    "    orient = np.expand_dims(np.array(orient), axis = 1)\n",
    "\n",
    "    instance_features = np.concatenate((X, Y, cell, cell_width, cell_height, orient), axis = 1)\n",
    "    # print(instance_features)\n",
    "\n",
    "    connection_fn = folder + design + '_connectivity.npz'\n",
    "    connection_data = np.load(connection_fn)\n",
    "    print('Connection info:', connection_fn)\n",
    "    \n",
    "    # get the direction of each edge between inst and net\n",
    "    dirs = []\n",
    "    edge_t = connection_data['data']\n",
    "    instance_idx = connection_data['row']\n",
    "    \n",
    "    for idx in range(len(instance_idx)):\n",
    "        inst = instance_idx[idx]\n",
    "        cell = inst_to_cell[inst]\n",
    "        edge_dict = cell_to_edge_dict[cell]\n",
    "        t = edge_t[idx]\n",
    "        direction = edge_dict[t]\n",
    "        dirs.append(direction)\n",
    "\n",
    "    dirs = np.array(dirs)\n",
    "\n",
    "    driver_sink_map = defaultdict(lambda: (None, []))\n",
    "\n",
    "    # Extract unique nodes and edges\n",
    "    nodes = list(set(connection_data['row']))\n",
    "    edges = list(set(connection_data['col']))\n",
    "\n",
    "    # Populate driver_sink_map\n",
    "    for node, edge, direction in zip(connection_data['row'], connection_data['col'], dirs):\n",
    "        if direction == 1:  # Driver\n",
    "            driver_sink_map[edge] = (node, driver_sink_map[edge][1])\n",
    "        elif direction == 0:  # Sink\n",
    "            driver_sink_map[edge][1].append(node)\n",
    "\n",
    "    # Convert to standard dictionary\n",
    "    driver_sink_map = dict(driver_sink_map)\n",
    "    print(driver_sink_map)\n",
    "\n",
    "    net_features = {}\n",
    "    for k, v in driver_sink_map.items():\n",
    "        if v[0]:\n",
    "            net_features[k] = [len(v[1]) + 1]\n",
    "        else:\n",
    "            net_features[k] = [len(v[1])]\n",
    "\n",
    "    instance_idx = connection_data['row']\n",
    "    net_idx = connection_data['col']\n",
    "    net_idx += num_instances\n",
    "\n",
    "    v1 = torch.unsqueeze(torch.Tensor(np.concatenate([instance_idx, net_idx], axis = 0)).long(), dim = 1)\n",
    "    v2 = torch.unsqueeze(torch.Tensor(np.concatenate([net_idx, instance_idx], axis = 0)).long(), dim = 1)\n",
    "    undir_edge_index = torch.transpose(torch.cat([v1, v2], dim = 1), 0, 1)\n",
    "\n",
    "    L = to_scipy_sparse_matrix(\n",
    "        *get_laplacian(undir_edge_index, normalization = \"sym\", num_nodes = num_instances + num_nets)\n",
    "    )\n",
    "    evals, evects = eigsh(L, k = 10, which='SM')\n",
    "    print(evects.shape)\n",
    "\n",
    "    node_features = {}\n",
    "    for i in range(num_instances):\n",
    "        node_features[i] = np.concatenate([instance_features[i, 2:], evects[i]])\n",
    "\n",
    "    print(net_features)\n",
    "    print(node_features)\n",
    "\n",
    "    congestion_fn = folder + design + '_congestion.npz'\n",
    "    congestion_data = np.load(congestion_fn)\n",
    "    print('Congestion info:', congestion_fn)\n",
    "\n",
    "    congestion_data_demand = congestion_data['demand']\n",
    "    congestion_data_capacity = congestion_data['capacity']\n",
    "\n",
    "    num_layers = len(list(congestion_data['layerList']))\n",
    "    print('Number of layers:', num_layers)\n",
    "    print('Layers:', list(congestion_data['layerList']))\n",
    "\n",
    "    ybl = congestion_data['yBoundaryList']\n",
    "    xbl = congestion_data['xBoundaryList']\n",
    "\n",
    "    all_demand = []\n",
    "    all_capacity = []\n",
    "\n",
    "    for layer in list(congestion_data['layerList']):\n",
    "        # print('Layer', layer, ':')\n",
    "        lyr = list(congestion_data['layerList']).index(layer)\n",
    "\n",
    "        # Binned statistics 2D\n",
    "        ret = binned_statistic_2d(xloc_list, yloc_list, None, 'count', bins = [xbl[1:], ybl[1:]], expand_binnumbers = True)\n",
    "\n",
    "        i_list = np.array([ret.binnumber[0, idx] - 1 for idx in range(num_instances)])\n",
    "        j_list = np.array([ret.binnumber[1, idx] - 1 for idx in range(num_instances)])\n",
    "\n",
    "        # Get demand and capacity\n",
    "        demand_list = congestion_data_demand[lyr, i_list, j_list].flatten()\n",
    "        capacity_list = congestion_data_capacity[lyr, i_list, j_list].flatten()\n",
    "\n",
    "        demand_list = np.array(demand_list)\n",
    "        capacity_list = np.array(capacity_list)\n",
    "\n",
    "        all_demand.append(np.expand_dims(demand_list, axis = 1))\n",
    "        all_capacity.append(np.expand_dims(capacity_list, axis = 1))\n",
    "\n",
    "        average_demand = np.mean(demand_list)\n",
    "        average_capacity = np.mean(capacity_list)\n",
    "        average_diff = np.mean(capacity_list - demand_list)\n",
    "        count_congestions = np.sum(demand_list > capacity_list)\n",
    "\n",
    "    demand = np.concatenate(all_demand, axis = 1).sum(axis=1)\n",
    "    capacity = np.concatenate(all_capacity, axis = 1).sum(axis=1)\n",
    "\n",
    "    congestion_actual = {}\n",
    "    for i in range(len(node_features)):\n",
    "        congestion_actual[i] = int(((capacity[i] * 0.9) - demand[i]) < 0)\n",
    "\n",
    "    with open(f'{clean_data_dir}{sample+1}.driver_sink_map.pkl', 'wb') as f:\n",
    "        pickle.dump(driver_sink_map, f)\n",
    "    \n",
    "    with open(f'{clean_data_dir}{sample+1}.node_features.pkl', 'wb') as f:\n",
    "        pickle.dump(node_features, f)\n",
    "\n",
    "    with open(f'{clean_data_dir}{sample+1}.net_features.pkl', 'wb') as f:\n",
    "        pickle.dump(net_features, f)\n",
    "\n",
    "    with open(f'{clean_data_dir}{sample+1}.congestion.pkl', 'wb') as f:\n",
    "        pickle.dump(congestion_actual, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Partition Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = raw_data_dir + 'xbar/'\n",
    "n_variants = 13\n",
    "num_partitions = 2\n",
    "\n",
    "for i in range(1, n_variants+1):\n",
    "    connection_data = np.load(f'{data_dir}{i}/xbar_connectivity.npz')\n",
    "\n",
    "    num_nodes = max(connection_data['row']) + 1\n",
    "    num_nets = max(connection_data['col']) + 1\n",
    "\n",
    "    # Convert hypergraph to bipartite graph representation\n",
    "    adj_list = [[] for _ in range(num_nodes + num_nets)]\n",
    "    for node, net in zip(connection_data['row'], connection_data['col']):\n",
    "        adj_list[node].append(num_nodes + net)\n",
    "        adj_list[num_nodes + net].append(node)\n",
    "\n",
    "    cuts, membership = pymetis.part_graph(num_partitions, adjacency=adj_list)\n",
    "    arr = np.array(membership[:num_nodes])\n",
    "\n",
    "    np.save(f'{clean_data_dir}{i}.partition.npy', arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEHNN Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DEHNNLayer(nn.Module):\n",
    "    def __init__(self, node_in_features, edge_in_features):\n",
    "        super(DEHNNLayer, self).__init__()\n",
    "        self.node_mlp1 = nn.Sequential(\n",
    "            nn.Linear(edge_in_features, edge_in_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.edge_mlp2 = nn.Sequential(\n",
    "            nn.Linear(node_in_features, node_in_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.edge_mlp3 = nn.Sequential(\n",
    "            nn.Linear(2 * node_in_features, 2 * node_in_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.node_to_virtual_mlp = nn.Sequential(\n",
    "            nn.Linear(node_in_features, node_in_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.virtual_to_higher_virtual_mlp = nn.Sequential(\n",
    "            nn.Linear(node_in_features, edge_in_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.higher_virtual_to_virtual_mlp = nn.Sequential(\n",
    "            nn.Linear(edge_in_features, edge_in_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.virtual_to_node_mlp = nn.Sequential(\n",
    "            nn.Linear(edge_in_features, edge_in_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Learnable defaults for missing driver or sink\n",
    "        self.default_driver = nn.Parameter(torch.zeros(node_in_features))\n",
    "        self.default_sink_agg = nn.Parameter(torch.zeros(node_in_features))\n",
    "        self.default_edge_agg = nn.Parameter(torch.zeros(edge_in_features))\n",
    "        self.default_virtual_node = nn.Parameter(torch.zeros(node_in_features))\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize all parameters with Xavier uniform distribution.\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, node_features, edge_features, hypergraph):\n",
    "        # Node update\n",
    "        updated_node_features = {}\n",
    "        for node in hypergraph.nodes:\n",
    "            incident_edges = hypergraph.get_incident_edges(node)\n",
    "            if incident_edges:\n",
    "                agg_features = torch.sum(torch.stack([self.node_mlp1(edge_features[edge]) for edge in incident_edges]), dim=0)\n",
    "            else:\n",
    "                agg_features = self.default_edge_agg\n",
    "            updated_node_features[node] = agg_features\n",
    "\n",
    "        # Edge update\n",
    "        updated_edge_features = {}\n",
    "        for edge in hypergraph.edges:\n",
    "            driver, sinks = hypergraph.get_driver_and_sinks(edge)\n",
    "\n",
    "            driver_feature = node_features[driver] if driver is not None else self.default_driver\n",
    "\n",
    "            if sinks:\n",
    "                sink_agg = torch.sum(torch.stack([self.edge_mlp2(node_features[sink]) for sink in sinks]), dim=0)\n",
    "            else:\n",
    "                sink_agg = self.default_sink_agg\n",
    "\n",
    "            concatenated = torch.cat([driver_feature, sink_agg])\n",
    "            updated_edge_features[edge] = self.edge_mlp3(concatenated)\n",
    "\n",
    "        # Virtual node aggregation\n",
    "        virtual_node_agg = {}\n",
    "        for virtual_node in range(hypergraph.num_virtual_nodes):\n",
    "            assigned_nodes = [node for node in hypergraph.nodes if hypergraph.get_virtual_node(node) == virtual_node]\n",
    "            if assigned_nodes:\n",
    "                agg_features = torch.sum(torch.stack([self.node_to_virtual_mlp(node_features[node]) for node in assigned_nodes]), dim=0)\n",
    "            else:\n",
    "                agg_features = self.default_virtual_node\n",
    "            virtual_node_agg[virtual_node] = agg_features\n",
    "\n",
    "        higher_virtual_feature = torch.sum(\n",
    "            torch.stack([self.virtual_to_higher_virtual_mlp(virtual_node_agg[vn]) for vn in virtual_node_agg]), dim=0\n",
    "        )\n",
    "\n",
    "        propagated_virtual_node_features = {}\n",
    "        for virtual_node in range(hypergraph.num_virtual_nodes):\n",
    "            propagated_virtual_node_features[virtual_node] = self.higher_virtual_to_virtual_mlp(higher_virtual_feature)\n",
    "\n",
    "        for node in hypergraph.nodes:\n",
    "            virtual_node = hypergraph.get_virtual_node(node)\n",
    "            propagated_feature = self.virtual_to_node_mlp(propagated_virtual_node_features[virtual_node])\n",
    "            updated_node_features[node] += propagated_feature\n",
    "\n",
    "        return updated_node_features, updated_edge_features\n",
    "\n",
    "\n",
    "class DEHNN(nn.Module):\n",
    "    def __init__(self, num_layers, node_in_features, edge_in_features):\n",
    "        super(DEHNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            self.layers.append(DEHNNLayer(node_in_features, edge_in_features))\n",
    "            node_in_features, edge_in_features = edge_in_features, node_in_features\n",
    "            edge_in_features *= 2\n",
    "\n",
    "        edge_in_features = edge_in_features // 2\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(node_in_features, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, node_features, edge_features, hypergraph):\n",
    "        for layer in self.layers:\n",
    "            node_features, edge_features = layer(node_features, edge_features, hypergraph)\n",
    "        \n",
    "        final_node_features = torch.stack([node_features[node] for node in hypergraph.nodes], dim=0)\n",
    "        output = self.output_layer(final_node_features)\n",
    "        return output\n",
    "    \n",
    "class Hypergraph:\n",
    "    def __init__(self, nodes, edges, driver_sink_map, node_to_virtual_map, num_virtual_nodes):\n",
    "        self.nodes = nodes\n",
    "        self.edges = edges\n",
    "        self.driver_sink_map = driver_sink_map\n",
    "        self.node_to_virtual_map = node_to_virtual_map\n",
    "        self.num_virtual_nodes = num_virtual_nodes\n",
    "\n",
    "    def get_incident_edges(self, node):\n",
    "        return [edge for edge in self.edges if node in self.driver_sink_map[edge][1] or node == self.driver_sink_map[edge][0]]\n",
    "\n",
    "    def get_driver_and_sinks(self, edge):\n",
    "        return self.driver_sink_map[edge]\n",
    "    \n",
    "    def get_virtual_node(self, node):\n",
    "        return self.node_to_virtual_map[node]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Design 1 Train Data and Hypergraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading training data and constructing hypergraph\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "with open(clean_data_dir + '1.driver_sink_map.pkl', 'rb') as f:\n",
    "    train_driver_sink_map = pickle.load(f)\n",
    "\n",
    "with open(clean_data_dir + '1.node_features.pkl', 'rb') as f:\n",
    "    train_node_features = pickle.load(f)\n",
    "\n",
    "with open(clean_data_dir + '1.net_features.pkl', 'rb') as f:\n",
    "    train_edge_features = pickle.load(f)\n",
    "\n",
    "with open(clean_data_dir + '1.congestion.pkl', 'rb') as f:\n",
    "    train_congestion = pickle.load(f)\n",
    "\n",
    "train_partition = np.load(clean_data_dir + '1.partition.npy')\n",
    "\n",
    "train_node_features = {k: torch.tensor(v).float().to(device) for k, v in train_node_features.items()}\n",
    "train_edge_features = {k: torch.tensor(v).float().to(device) for k, v in train_edge_features.items()}\n",
    "\n",
    "train_nodes = list(range(len(train_node_features)))\n",
    "train_edges = list(range(len(train_edge_features)))\n",
    "train_hypergraph = Hypergraph(train_nodes, train_edges, train_driver_sink_map, train_partition, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Design 2 Validation Data and Hypergraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading validation data and constructing hypergraph\n",
    "with open(clean_data_dir + '2.driver_sink_map.pkl', 'rb') as f:\n",
    "    val_driver_sink_map = pickle.load(f)\n",
    "\n",
    "with open(clean_data_dir + '2.node_features.pkl', 'rb') as f:\n",
    "    val_node_features = pickle.load(f)\n",
    "\n",
    "with open(clean_data_dir + '2.net_features.pkl', 'rb') as f:\n",
    "    val_edge_features = pickle.load(f)\n",
    "\n",
    "with open(clean_data_dir + '2.congestion.pkl', 'rb') as f:\n",
    "    val_congestion = pickle.load(f)\n",
    "\n",
    "val_partition = np.load(clean_data_dir + '2.partition.npy')\n",
    "\n",
    "val_node_features = {k: torch.tensor(v).float().to(device) for k, v in val_node_features.items()}\n",
    "val_edge_features = {k: torch.tensor(v).float().to(device) for k, v in val_edge_features.items()}\n",
    "\n",
    "val_nodes = list(range(len(val_node_features)))\n",
    "val_edges = list(range(len(val_edge_features)))\n",
    "val_hypergraph = Hypergraph(val_nodes, val_edges, val_driver_sink_map, val_partition, 2)\n",
    "val_targets = val_congestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on Design 1, Validate on Design 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DEHNN(num_layers=4, node_in_features=14, edge_in_features=1).to(device)\n",
    "epochs = 20\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "train_node_features = {k: v.to(device) for k, v in train_node_features.items()}\n",
    "train_edge_features = {k: v.to(device) for k, v in train_edge_features.items()}\n",
    "train_targets = torch.tensor(list(train_congestion.values())).long().to(device)\n",
    "\n",
    "val_node_features = {k: v.to(device) for k, v in val_node_features.items()}\n",
    "val_edge_features = {k: v.to(device) for k, v in val_edge_features.items()}\n",
    "val_targets = torch.tensor(list(val_congestion.values())).long().to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    train_output = model(train_node_features, train_edge_features, train_hypergraph)\n",
    "    \n",
    "    train_loss = criterion(train_output, train_targets)\n",
    "    \n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(val_node_features, val_edge_features, val_hypergraph)\n",
    "        \n",
    "        val_loss = criterion(val_output, val_targets)\n",
    "        \n",
    "        val_predictions = torch.argmax(val_output, dim=1)\n",
    "        val_correct = (val_predictions == val_targets).sum().item()\n",
    "        val_total = len(val_targets)\n",
    "        val_accuracy = val_correct / val_total\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{epochs}]\")\n",
    "    print(f\"Train Loss: {train_loss.item():.4f}\")\n",
    "    print(f\"Validation Loss: {val_loss.item():.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train across multiple designs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_indices = range(1, 9)\n",
    "epochs = 10\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i in file_indices:\n",
    "        print(f'Chip {i}:')\n",
    "        \n",
    "        with open(f'{clean_data_dir}{i}.driver_sink_map.pkl', 'rb') as f:\n",
    "            driver_sink_map = pickle.load(f)\n",
    "        \n",
    "        with open(f'{clean_data_dir}{i}.node_features.pkl', 'rb') as f:\n",
    "            node_features = pickle.load(f)\n",
    "        \n",
    "        with open(f'{clean_data_dir}{i}.net_features.pkl', 'rb') as f:\n",
    "            edge_features = pickle.load(f)\n",
    "        \n",
    "        with open(f'{clean_data_dir}{i}.congestion.pkl', 'rb') as f:\n",
    "            congestion = pickle.load(f)\n",
    "        \n",
    "        partition = np.load(f'{clean_data_dir}{i}.partition.npy')\n",
    "        \n",
    "        node_features = {k: torch.tensor(v).float().to(device) for k, v in node_features.items()}\n",
    "        edge_features = {k: torch.tensor(v).float().to(device) for k, v in edge_features.items()}\n",
    "        \n",
    "        nodes = list(range(len(node_features)))\n",
    "        edges = list(range(len(edge_features)))\n",
    "        hypergraph = Hypergraph(nodes, edges, driver_sink_map, partition, 2)\n",
    "        \n",
    "        output = model(node_features, edge_features, hypergraph)\n",
    "        \n",
    "        target = torch.tensor(list(congestion.values())).to(device)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        print(f'Epoch [{epoch+1}/10], Loss: {epoch_loss:.4f}')\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/10], Loss: {epoch_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
